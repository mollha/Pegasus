{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "pegasus.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rgXLI4iSSEv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "DCGAN with Spectral Normalisation Layers\n",
        "==============\n",
        "`https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html`\n",
        "\n",
        "Based on the Pytorch tutorial by Nathan Inkawhich `<https://github.com/inkawhich>`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TfcPMnTCfy",
        "colab_type": "text"
      },
      "source": [
        "**Define imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl6bPLPxSSEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install -q torch torchvision livelossplot\n",
        "!pip install torchsummary\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "from os.path import isfile\n",
        "import random\n",
        "from matplotlib.colors import Normalize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "from time import sleep\n",
        "import torchvision\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torchsummary import summary\n",
        "import torchvision.datasets as dset\n",
        "from livelossplot import PlotLosses\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as plt_image\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpkwRSPGcSZt",
        "colab_type": "text"
      },
      "source": [
        "**Define a class for the Custom Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BEcPoU7cWe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(CIFAR10):\n",
        "    def __init__(self, root: str, class_names: list, train=True, transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "\n",
        "      super().__init__(root, transform=transform, target_transform=target_transform, download=download)\n",
        "\n",
        "      self.image_classes = [self.get_index(name) for name in class_names]\n",
        "      self.targets, self.data = self.get_classes(self.image_classes)\n",
        "\n",
        "    def get_index(self, class_name: str) -> int:\n",
        "      name_dict = {'plane': 0, 'bird': 2, 'horse': 7}\n",
        "      return name_dict[class_name]\n",
        "\n",
        "    def get_classes(self, class_indexes: set):\n",
        "      class_dict = {i:[] for i in class_indexes}\n",
        "      for index in range(len(self.data)):\n",
        "        target = self.targets[index]      \n",
        "\n",
        "        if target in class_indexes:\n",
        "          image = self.data[index]\n",
        "          class_dict[target].append(image)\n",
        "\n",
        "      data = np.concatenate([np.array(class_dict[ind]) for ind in class_dict])\n",
        "      targets = np.concatenate([np.array([ind]*len(class_dict[ind])) for ind in class_dict])\n",
        "      return targets, data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG_kqOQYSSE1",
        "colab_type": "text"
      },
      "source": [
        "Define Settings\n",
        "------\n",
        "\n",
        "-  **batch_size** - the batch size used in training. The DCGAN paper\n",
        "   uses a batch size of 128\n",
        "-  **num_epochs** - number of training epochs to run. Training for\n",
        "   longer will probably lead to better results but will also take much\n",
        "   longer\n",
        "-  **lr** - learning rate for training. As described in the DCGAN paper,\n",
        "   this number should be 0.0002\n",
        "-  **beta1** - beta1 hyperparameter for Adam optimizers. As described in\n",
        "   paper, this number should be 0.5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQAGrXIqSSE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size during training\n",
        "batch_size = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 50\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZj-XW071krG",
        "colab_type": "text"
      },
      "source": [
        "Prepare Data\n",
        "------\n",
        "\n",
        "- **select_classes** - list of classes to train on\n",
        "- valid combinations include ['bird', 'horse'] and ['plane', 'horse']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygYPzYfOSSE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to make getting another batch of data easier\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "\n",
        "class_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "select_classes = ['plane', 'horse']\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "training_set = CustomDataset('root', select_classes, train=True, download=True, transform=train_transform)\n",
        "testing_set = CustomDataset('root', select_classes, train=False, download=True, transform=train_transform)\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(training_set, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing_set, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "data_iterator = iter(cycle(train_loader))\n",
        "print('Dataset contains the following image classes: ' + str(select_classes))\n",
        "print(f'> Size of dataset {len(train_loader.dataset) + len(test_loader.dataset)}')\n",
        "\n",
        "# Visualise some of the data\n",
        "plt.figure(figsize=(10,10))\n",
        "images, labels = next(data_iterator)\n",
        "\n",
        "for i in range(64):\n",
        "  plt.subplot(8,8,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(images[i].permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
        "  plt.xlabel(class_names[labels[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHgIVT_GSSE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom weights initialization called on generator_network and discriminator_network\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWTI-e5B4zRE",
        "colab_type": "text"
      },
      "source": [
        "Define Models\n",
        "-------------\n",
        "Create the Generator and Discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgD71o98SSFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, f=32):\n",
        "        super(Generator, self).__init__()\n",
        "        self.generate = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( 100, f * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(f * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (f*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(f * 8, f * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(f * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (f*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( f * 4, f * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(f * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (f*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( f * 2, f, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(f),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (f) x 32 x 32\n",
        "            nn.ConvTranspose2d( f, 3, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (3) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.generate(input)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, f=32):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminate = nn.Sequential(\n",
        "            # input is (3) x 64 x 64\n",
        "            torch.nn.utils.spectral_norm(nn.Conv2d(3, f, 4, 2, 1, bias=False)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (f) x 32 x 32\n",
        "            torch.nn.utils.spectral_norm(nn.Conv2d(f, f * 2, 4, 2, 1, bias=False)),\n",
        "            nn.BatchNorm2d(f * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (f*2) x 16 x 16\n",
        "            torch.nn.utils.spectral_norm(nn.Conv2d(f * 2, f * 4, 4, 2, 1, bias=False)),\n",
        "            nn.BatchNorm2d(f * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (f*4) x 8 x 8\n",
        "            torch.nn.utils.spectral_norm(nn.Conv2d(f * 4, f * 8, 4, 2, 1, bias=False)),\n",
        "            nn.BatchNorm2d(f * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (f*8) x 4 x 4\n",
        "            torch.nn.utils.spectral_norm(nn.Conv2d(f * 8, 1, 2, 2, 0, bias=False)),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.discriminate(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQjVEVigSSFI",
        "colab_type": "text"
      },
      "source": [
        "Instantiate the generator and apply the ``weights_init``\n",
        "function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLU1VELSSFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the generator and discriminator\n",
        "discriminator_network = Discriminator().to(device)\n",
        "generator_network = Generator().to(device)\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "generator_network.apply(weights_init)\n",
        "discriminator_network.apply(weights_init)\n",
        "\n",
        "# Print the models\n",
        "print('------------------ Generator Architecture -------------------')\n",
        "print(generator_network)\n",
        "summary(generator_network, input_size=(100, 1, 1))\n",
        "\n",
        "print('\\n------------------ Discriminator Architecture -------------------')\n",
        "print(discriminator_network)\n",
        "summary(discriminator_network, input_size=(3, 32, 32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vkp-mpGMJhO",
        "colab_type": "text"
      },
      "source": [
        "Create checkpoints to save weights during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT6pVFtPMMf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(dis, gen, d_opt, g_opt, epoch, d_losses, g_losses, gen_loss_arr, dis_loss_arr, plot):\n",
        "  check_point = {'epoch': epoch + 1,\n",
        "  'd_model_state': dis.state_dict(),\n",
        "  'g_model_state': gen.state_dict(),\n",
        "  'd_optimiser_state': d_opt.state_dict(),\n",
        "  'g_optimiser_state': g_opt.state_dict(),\n",
        "  'd_losses': d_losses,\n",
        "  'g_losses': g_losses,\n",
        "  'live_g': gen_loss_arr,\n",
        "  'live_d': dis_loss_arr,\n",
        "  'plot': plot\n",
        "  }\n",
        "  torch.save(check_point, 'checkpoint.tar')\n",
        "  return check_point\n",
        "\n",
        "def load_checkpoint(discriminator, generator, optimiser_D, optimiser_G):\n",
        "  check_point = torch.load('checkpoint.tar')\n",
        "  epoch = check_point['epoch']\n",
        "  discriminator.load_state_dict(check_point['d_model_state'])\n",
        "  generator.load_state_dict(check_point['g_model_state'])\n",
        "  optimiser_G.load_state_dict(check_point['g_optimiser_state'])\n",
        "  optimiser_D.load_state_dict(check_point['d_optimiser_state'])\n",
        "  d_losses, g_losses = check_point['d_losses'], check_point['g_losses']\n",
        "  gen_loss_arr, dis_loss_arr = check_point['live_g'], check_point['live_d']\n",
        "  plot = check_point['plot']\n",
        "  return epoch, d_losses, g_losses, gen_loss_arr, dis_loss_arr, plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCR1DU0oYd4u",
        "colab_type": "text"
      },
      "source": [
        "**Create the Optimisers and Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6-u9-FkSSFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize BCELoss function\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(discriminator_network.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(generator_network.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ImjgPqCUhK4",
        "colab_type": "text"
      },
      "source": [
        "**Main Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq1pdb3-SSFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lists to keep track of progress\n",
        "liveplot = PlotLosses()\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "# number of times to train discriminator before training generator\n",
        "step = 5\n",
        "\n",
        "def train(data_loaders: list, num_epochs: int, liveplot, epoch=0, settings=None):\n",
        "\n",
        "  if settings is not None:\n",
        "    epoch, errD, errG = settings['epoch'], settings['D_losses'], settings['G_losses']\n",
        "    gen_loss_arr, dis_loss_arr = settings['gen_loss_arr'], settings['dis_loss_arr']\n",
        "    liveplot = settings['plot']\n",
        "\n",
        "  \n",
        "  print(\"Starting Training Loop...\")\n",
        "  # For each epoch\n",
        "  for epoch in range(epoch, num_epochs):\n",
        "      \n",
        "      def iteration(loader, gen_loss_arr, dis_loss_arr):\n",
        "        for i, data in enumerate(loader, 0):\n",
        "\n",
        "            #----------------------------------------------------\n",
        "            # Update Discriminator network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "            ## Train with all-real batch\n",
        "            discriminator_network.zero_grad()\n",
        "            # Format batch\n",
        "            real_cpu = data[0].to(device)\n",
        "            b_size = real_cpu.size(0)\n",
        "            label = torch.full((b_size,), 1, device=device)\n",
        "            # Forward pass real batch through D\n",
        "            output = discriminator_network(real_cpu).view(-1)\n",
        "            # Calculate loss on all-real batch\n",
        "            errD_real = loss_function(output, label)\n",
        "            # Calculate gradients for D in backward pass\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            ## Train with all-fake batch\n",
        "            # Generate batch of latent vectors\n",
        "            noise = torch.randn(b_size, 100, 1, 1, device=device)\n",
        "            # Generate fake image batch with G\n",
        "            fake_batch = generator_network(noise)\n",
        "            label.fill_(0)  # fake label\n",
        "            # Classify all fake batch with D\n",
        "            output = discriminator_network(fake_batch.detach()).view(-1)\n",
        "            # Calculate D's loss on the all-fake batch\n",
        "            errD_fake = loss_function(output, label)\n",
        "            # Calculate the gradients for this batch\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            # Add the gradients from the all-real and all-fake batches\n",
        "            errD = errD_real + errD_fake\n",
        "            # Update D\n",
        "            optimizerD.step()\n",
        "\n",
        "            if i % step == 0:\n",
        "                #----------------------------------------------------\n",
        "                # Update Generator network: maximize log(D(G(z)))\n",
        "                generator_network.zero_grad()\n",
        "                label.fill_(1)  # fake labels are real for generator cost\n",
        "                # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "                output = discriminator_network(fake_batch).view(-1)\n",
        "                # Calculate G's loss based on this output\n",
        "                errG = loss_function(output, label)\n",
        "                # Calculate gradients for G\n",
        "                errG.backward()\n",
        "                D_G_z2 = output.mean().item()\n",
        "                # Update G\n",
        "                optimizerG.step()\n",
        "\n",
        "                # Output training stats\n",
        "                if i % (step * 10) == 0:\n",
        "                    tl = sum([len(lo) for lo in data_loaders])\n",
        "                    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                          % (epoch, num_epochs, i, tl,\n",
        "                            errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))    \n",
        "                    \n",
        "                    # Save Losses for plotting late\n",
        "                    G_losses.append(errG.item())\n",
        "                    D_losses.append(errD.item())\n",
        "\n",
        "        \n",
        "                    gen_loss_arr = np.append(gen_loss_arr, errG.item())\n",
        "                    dis_loss_arr = np.append(dis_loss_arr, errD.item())\n",
        "        \n",
        "        return gen_loss_arr, dis_loss_arr\n",
        "                        \n",
        "      gen_loss_arr = np.zeros(0)\n",
        "      dis_loss_arr = np.zeros(0)\n",
        "      \n",
        "      # Iterate over each data loader in a single epoch (the training and testing set)\n",
        "      for loader in data_loaders:\n",
        "        gen_loss_arr, dis_loss_arr = iteration(loader, gen_loss_arr, dis_loss_arr)\n",
        "\n",
        "      # Update the graph after each epoch\n",
        "      liveplot.update({\n",
        "      'Generator loss': gen_loss_arr.mean(),\n",
        "      'Discriminator loss': dis_loss_arr.mean()\n",
        "      })\n",
        "      liveplot.draw()\n",
        "      sleep(1.)\n",
        "\n",
        "      # Check how the generator is doing by saving G's output on noise\n",
        "      with torch.no_grad():\n",
        "          fake = generator_network(torch.randn(64, 100, 1, 1, device=device)).detach().cpu()\n",
        "\n",
        "      # Plot generator images (created from random noise) after each epoch\n",
        "      g = generator_network.generate(torch.randn(fake.size(0), 100, 1, 1).to(device))\n",
        "      plt.grid(False)\n",
        "      plt.imshow(torchvision.utils.make_grid(g).cpu().data.permute(0,2,1).contiguous().permute(2,1,0).clamp(0,1), cmap=plt.cm.binary)\n",
        "\n",
        "      # save model weights after each epoch\n",
        "      save_checkpoint(discriminator_network, generator_network, optimizerD, optimizerG, epoch, D_losses, G_losses, gen_loss_arr, dis_loss_arr, liveplot)\n",
        "    \n",
        "\n",
        "## ------ READ IN CHECKPOINT FILE IF IT EXISTS --------\n",
        "if isfile('checkpoint.tar'):\n",
        "  epoch, loss_d, loss_g, gen_loss_arr, dis_loss_arr, liveplot = load_checkpoint(discriminator_network, generator_network, optimizerD, optimizerG)\n",
        "  settings = {'epoch': epoch, 'D_losses':loss_d, 'G_losses':loss_g, 'gen_loss_arr':gen_loss_arr, 'dis_loss_arr': dis_loss_arr, 'plot': liveplot}\n",
        "else:\n",
        "  settings = None\n",
        "\n",
        "train([train_loader, test_loader], 250, liveplot, settings=settings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXs36n5NSSFh",
        "colab_type": "text"
      },
      "source": [
        "**Plot the losses and for each training iteration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyepG2sDSSFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUqBiKXRSSFs",
        "colab_type": "text"
      },
      "source": [
        "**Plot the fake images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EovuXKhJSSFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "g = generator_network(torch.randn(64, 100, 1, 1, device=device)).detach().cpu()\n",
        "img_list = [g[i].permute(0,2,1).contiguous().permute(2,1,0).clamp(0,1)]\n",
        "\n",
        "for i in range(64):\n",
        "  plt.subplot(8,8,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(g[i].permute(0,2,1).contiguous().permute(2,1,0).clamp(0,1), cmap=plt.cm.binary)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}